{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pickle,pprint\n",
    "import os\n",
    "from scipy.misc import imread\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "num_pic = 2500\n",
    "lst_labels = []\n",
    "\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    return np.eye(2)[x]\n",
    "\n",
    "\n",
    "# function for reading the images\n",
    "# arguments: path to the photos, for example 'pics'\n",
    "# returns: ndarray of images\n",
    "def readPhotos(rootpath):\n",
    "\n",
    "    idx = 0\n",
    "    IMG = np.ones((num_pic,227,227,3))     # TOTAL No.images and size of images\n",
    "\n",
    "\n",
    "    for root, dirs, files in os.walk(rootpath):\n",
    "        for file in files:\n",
    "            photo_path = os.path.join(rootpath,file)      # photo path\n",
    "            #print(photo_path)\n",
    "            img = (imread(photo_path)[:,:,:3]).astype(np.float32)    # read to ndarray\n",
    "            resized_img = cv2.resize(img, (227,227))\n",
    "            #norm_img = resized_img - np.mean(resized_img)  # normalize the image\n",
    "            norm_img = (resized_img - np.min(resized_img)) / (np.max(resized_img) - np.min(resized_img))\n",
    "\n",
    "            #plt.show(norm_img)\n",
    "            IMG[idx] = norm_img\n",
    "\n",
    "            if 'cat' in file:\n",
    "                lst_labels.append(0)    # the photo is cat\n",
    "            else:\n",
    "                lst_labels.append(1)    # the photo is dog\n",
    "\n",
    "            idx = idx + 1\n",
    "    return IMG\n",
    "\n",
    "\n",
    "image = readPhotos('train_1')\n",
    "label = one_hot_encode(lst_labels)\n",
    "# print(type(image))\n",
    "# print(image.size)\n",
    "# print(image)\n",
    "\n",
    "#print(\"labels:\",label)\n",
    "fffff = [['features',image], ['label', label]]\n",
    "\n",
    "with open('batch_1.pickle', 'wb') as f:\n",
    "    # Pickle the 'features' dictionary using the highest protocol available.\n",
    "    pickle.dump(fffff, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "print('data completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "#from alexnet import AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = 43\n",
    "epochs = 1\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './train.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-394fc50a8a78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./train.p'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './train.p'"
     ]
    }
   ],
   "source": [
    "with open('./data.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split the data into training data set and test data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AlexNet only accepts data with 227*227, while pre-loaded data is 32*32, reshape the size so that AlexNet can accept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(data['features'], data['labels'], test_size=0.33, random_state=0)\n",
    "\n",
    "features = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "labels = tf.placeholder(tf.int64, None)\n",
    "resized = tf.image.resize_images(features, (227, 227))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return the 7th layer data from AlexNet, and modify the final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fc7 = AlexNet(resized, feature_extract=True)\n",
    "fc7 = tf.stop_gradient(fc7)\n",
    "shape = (fc7.get_shape().as_list()[-1], nb_classes)\n",
    "fc8W = tf.Variable(tf.truncated_normal(shape, stddev=1e-2))\n",
    "fc8b = tf.Variable(tf.zeros(nb_classes))\n",
    "logits = tf.nn.xw_plus_b(fc7, fc8W, fc8b)\n",
    "\n",
    "cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels)\n",
    "loss_op = tf.reduce_mean(cross_entropy)\n",
    "opt = tf.train.AdamOptimizer()\n",
    "train_op = opt.minimize(loss_op, var_list=[fc8W, fc8b])\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "preds = tf.arg_max(logits, 1)\n",
    "accuracy_op = tf.reduce_mean(tf.cast(tf.equal(preds, labels), tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
